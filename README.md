# train_free_comp

TODO

use KL divergence
there are multiple losses that could be used. Distillation loss on answer logits. Distillation on activations on answer tokens. Distillation on activations of all non-new tokens.



measure the embeddings which should not be used, like "+ 1" vs the ones to be used, like "and then".



